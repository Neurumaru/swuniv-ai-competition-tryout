{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import Input\n",
    "from keras import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import GaussianNoise\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.metrics import AUC\n",
    "from keras.regularizers import L1L2\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('mode.chained_assignment',  None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 'inputs'\n",
    "outputs = 'outputs'\n",
    "\n",
    "train = pd.read_csv(inputs + '/train.csv')\n",
    "test = pd.read_csv(inputs + '/test.csv')\n",
    "submission = pd.read_csv(outputs + '/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\pandas\\core\\arraylike.py:364: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_x = train.copy().iloc[:, :-1]\n",
    "train_y = train.copy()['nerdiness']\n",
    "\n",
    "\n",
    "def onehot_encoder(dataframe, target, encoder=None):\n",
    "    if encoder == None:\n",
    "        encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        encoder.fit(dataframe[[target]])\n",
    "    ohe_df = encoder.transform(dataframe[[target]])\n",
    "    tag_function = lambda x: f'{target}_{x}'\n",
    "    ohe_df = pd.DataFrame(ohe_df, columns=list(map(tag_function, encoder.categories_[0])))\n",
    "    results = pd.concat([dataframe, ohe_df], axis=1)\n",
    "    \n",
    "    return results, encoder\n",
    "\n",
    "\n",
    "def preprocess(x, copy=True, encoders=None, scaler=None):\n",
    "    if copy:\n",
    "        x = x.copy()\n",
    "\n",
    "    # cast\n",
    "    int_list = [\n",
    "        'VCL1', 'VCL2', 'VCL3', 'VCL4', 'VCL5', 'VCL6', 'VCL7', 'VCL8',\n",
    "        'VCL9', 'VCL10', 'VCL11', 'VCL12', 'VCL13', 'VCL14', 'VCL15', 'VCL16',\n",
    "        'urban', 'age'\n",
    "    ]\n",
    "    for col in int_list:\n",
    "        x[col] = x[col].astype(float)\n",
    "\n",
    "    # isnull (missing values)\n",
    "    null_list = [\n",
    "        'Q1', 'Q2', 'Q3', 'Q4', 'Q5','Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'Q11', 'Q12', 'Q13',\n",
    "        'Q14', 'Q15', 'Q16', 'Q17', 'Q18', 'Q19', 'Q20', 'Q21', 'Q22', 'Q23', 'Q24', 'Q25', 'Q26',\n",
    "        'TIPI1', 'TIPI2', 'TIPI3', 'TIPI4', 'TIPI5', 'TIPI6', 'TIPI7', 'TIPI8', 'TIPI9', 'TIPI10',\n",
    "        'education', 'gender', 'familysize']\n",
    "    for col in null_list:\n",
    "        x[f'{col}_isnull'] = np.where(pd.isnull(x[col]), 1.0, 0.0)\n",
    "\n",
    "    # mean (missing values)\n",
    "    mean_list = [\n",
    "        'Q1', 'Q2', 'Q3', 'Q4', 'Q5','Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'Q11', 'Q12', 'Q13',\n",
    "        'Q14', 'Q15', 'Q16', 'Q17', 'Q18', 'Q19', 'Q20', 'Q21', 'Q22', 'Q23', 'Q24', 'Q25', 'Q26',\n",
    "        'TIPI1', 'TIPI2', 'TIPI3', 'TIPI4', 'TIPI5', 'TIPI6', 'TIPI7', 'TIPI8', 'TIPI9', 'TIPI10',\n",
    "        'familysize']\n",
    "    for col in mean_list:\n",
    "        x[col][pd.isnull(x[col])] = x[col].mean()\n",
    "\n",
    "    # value (missing values)\n",
    "    value_dict = {\n",
    "        'country': 'None',\n",
    "        'education': 1.5,\n",
    "        'gender': 0,\n",
    "        'engnat': 0,\n",
    "        'hand': 1,\n",
    "        'religion': 0,\n",
    "        'orientation': 0,\n",
    "        'voted': 0,\n",
    "        'married': 0,\n",
    "        'ASD': 0}\n",
    "    for col in value_dict:\n",
    "        x[col][pd.isnull(x[col])] = value_dict[col]\n",
    "\n",
    "    # value (0 values)\n",
    "    value0_dict = {\n",
    "        'urban': 1.5\n",
    "    }\n",
    "    for col in value0_dict:\n",
    "        x[f'{col}_isnull'] = np.where(x[col] == 0, 1.0, 0.0)\n",
    "        x[col][x[col] == 0] = value0_dict[col]\n",
    "\n",
    "    # OneHotEncoder\n",
    "    ohe_list = [\n",
    "        'country', 'gender', 'engnat', 'hand', 'religion',\n",
    "        'orientation', 'voted', 'married', 'ASD']\n",
    "    if encoders == None:\n",
    "        encoders = dict()\n",
    "    for col in ohe_list:\n",
    "        if col in encoders:\n",
    "            x, encoders[col] = onehot_encoder(x, col, encoder=encoders[col])\n",
    "        else:\n",
    "            x, encoders[col] = onehot_encoder(x, col)\n",
    "\n",
    "    # log (outlier)\n",
    "    log_list = [\n",
    "        'introelapse', 'testelapse', 'surveyelapse'\n",
    "    ]\n",
    "    for col in log_list:\n",
    "        x[f'log_{col}'] = np.log(x[col] + 1e-08)\n",
    "\n",
    "    # min-max cut (outlier)\n",
    "    outlier_dict = {\n",
    "        'age': {'min': 1, 'max': 100},\n",
    "        'log_introelapse': {'min': 0, 'max': 12},\n",
    "        'log_testelapse': {'min': 3, 'max': 8},\n",
    "        'log_surveyelapse': {'min': 0, 'max': 10},\n",
    "        'familysize': {'min': 1, 'max': 40}}\n",
    "    for col in outlier_dict:\n",
    "        x[col][x[col] > outlier_dict[col]['max']] = outlier_dict[col]['max']\n",
    "        x[col][x[col] < outlier_dict[col]['min']] = outlier_dict[col]['min']\n",
    "\n",
    "    # drop\n",
    "    drop_list = [\n",
    "        'index', \n",
    "        'country_AGO', 'country_ALA', 'country_ARM', 'country_AZE',\n",
    "        'country_BHS', 'country_BLR', 'country_BRB', 'country_BRN',\n",
    "        'country_BWA', 'country_DOM', 'country_ETH', 'country_FRO',\n",
    "        'country_GRL', 'country_GTM', 'country_GUF', 'country_GUY',\n",
    "        'country_IRQ', 'country_KAZ', 'country_KHM', 'country_LBY',\n",
    "        'country_LUX', 'country_MAC', 'country_MDV', 'country_MNP',\n",
    "        'country_MOZ', 'country_MUS', 'country_MWI', 'country_NAM',\n",
    "        'country_NPL', 'country_OMN', 'country_PAN', 'country_SDN',\n",
    "        'country_SSD', 'country_TUN', 'country_UGA', 'country_VGB',\n",
    "        'country_VIR', 'country_FSM', 'country_GEO', 'country_PNG',\n",
    "        'country_RWA', 'country_SYR', 'country_LAO', 'country_MNG',\n",
    "        'country_CUW', 'country_MLT', 'country_BHR', 'country_MDG'\n",
    "    ] + log_list + ohe_list\n",
    "    for col in drop_list:\n",
    "        x = x.drop(columns=col)\n",
    "\n",
    "    if scaler == None:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(x)\n",
    "    x = pd.DataFrame(scaler.transform(x), columns=x.columns, index=list(x.index.values))\n",
    "\n",
    "    return x, encoders, scaler\n",
    "\n",
    "train_x, encoders, scaler = preprocess(train_x, copy=False)\n",
    "test_x, _, _ = preprocess(test, encoders=encoders, scaler=scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "grid_parameters = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 4],\n",
    "    'min_samples_leaf': [2, 4],\n",
    "    'min_weight_fraction_leaf': [0.0, 1e-5],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'bootstrap': [True, False],\n",
    "    'n_jobs': [8],\n",
    "    'class_weight': ['balanced', 'balanced_subsample', None],\n",
    "}\n",
    "\n",
    "grid_rfc = GridSearchCV(model, param_grid=grid_parameters, cv=5)\n",
    "grid_rfc.fit(train_x, train_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tensorflow2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0d89f1864a9bf14cab8f26b7808b33bb77607a1d02657216be50ded1f9f2bf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
