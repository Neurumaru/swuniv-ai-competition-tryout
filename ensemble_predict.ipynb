{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('mode.chained_assignment',  None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 'inputs'\n",
    "outputs = 'outputs'\n",
    "\n",
    "train = pd.read_csv(inputs + '/train.csv')\n",
    "target = pd.read_csv(inputs + '/test.csv')\n",
    "submission = pd.read_csv(outputs + '/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.copy().iloc[:, :-1]\n",
    "y_train = train.copy()['nerdiness']\n",
    "\n",
    "\n",
    "def onehot_encoder(dataframe, target, encoder=None):\n",
    "    if encoder == None:\n",
    "        encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        encoder.fit(dataframe[[target]])\n",
    "    ohe_df = encoder.transform(dataframe[[target]])\n",
    "    tag_function = lambda x: f'{target}_{x}'\n",
    "    ohe_df = pd.DataFrame(ohe_df, columns=list(map(tag_function, encoder.categories_[0])))\n",
    "    results = pd.concat([dataframe, ohe_df], axis=1)\n",
    "    \n",
    "    return results, encoder\n",
    "\n",
    "\n",
    "def preprocess(x, copy=True, encoders=None, scaler=None):\n",
    "    if copy:\n",
    "        x = x.copy()\n",
    "\n",
    "    # cast\n",
    "    int_list = [\n",
    "        'VCL1', 'VCL2', 'VCL3', 'VCL4', 'VCL5', 'VCL6', 'VCL7', 'VCL8',\n",
    "        'VCL9', 'VCL10', 'VCL11', 'VCL12', 'VCL13', 'VCL14', 'VCL15', 'VCL16',\n",
    "        'urban', 'age'\n",
    "    ]\n",
    "    for col in int_list:\n",
    "        x[col] = x[col].astype(float)\n",
    "\n",
    "    # isnull (missing values)\n",
    "    null_list = [\n",
    "        'Q1', 'Q2', 'Q3', 'Q4', 'Q5','Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'Q11', 'Q12', 'Q13',\n",
    "        'Q14', 'Q15', 'Q16', 'Q17', 'Q18', 'Q19', 'Q20', 'Q21', 'Q22', 'Q23', 'Q24', 'Q25', 'Q26',\n",
    "        'TIPI1', 'TIPI2', 'TIPI3', 'TIPI4', 'TIPI5', 'TIPI6', 'TIPI7', 'TIPI8', 'TIPI9', 'TIPI10',\n",
    "        'education', 'gender', 'familysize']\n",
    "    for col in null_list:\n",
    "        x[f'{col}_isnull'] = np.where(pd.isnull(x[col]), 1.0, 0.0)\n",
    "\n",
    "    # mean (missing values)\n",
    "    mean_list = [\n",
    "        'Q1', 'Q2', 'Q3', 'Q4', 'Q5','Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'Q11', 'Q12', 'Q13',\n",
    "        'Q14', 'Q15', 'Q16', 'Q17', 'Q18', 'Q19', 'Q20', 'Q21', 'Q22', 'Q23', 'Q24', 'Q25', 'Q26',\n",
    "        'TIPI1', 'TIPI2', 'TIPI3', 'TIPI4', 'TIPI5', 'TIPI6', 'TIPI7', 'TIPI8', 'TIPI9', 'TIPI10',\n",
    "        'familysize']\n",
    "    for col in mean_list:\n",
    "        x[col][pd.isnull(x[col])] = x[col].mean()\n",
    "\n",
    "    # value (missing values)\n",
    "    value_dict = {\n",
    "        'country': 'None',\n",
    "        'education': 1.5,\n",
    "        'gender': 0,\n",
    "        'engnat': 0,\n",
    "        'hand': 1,\n",
    "        'religion': 0,\n",
    "        'orientation': 0,\n",
    "        'voted': 0,\n",
    "        'married': 0,\n",
    "        'ASD': 0}\n",
    "    for col in value_dict:\n",
    "        x[col][pd.isnull(x[col])] = value_dict[col]\n",
    "\n",
    "    # value (0 values)\n",
    "    value0_dict = {\n",
    "        'urban': 1.5\n",
    "    }\n",
    "    for col in value0_dict:\n",
    "        x[f'{col}_isnull'] = np.where(x[col] == 0, 1.0, 0.0)\n",
    "        x[col][x[col] == 0] = value0_dict[col]\n",
    "\n",
    "    # OneHotEncoder\n",
    "    ohe_list = [\n",
    "        'country', 'gender', 'engnat', 'hand', 'religion',\n",
    "        'orientation', 'voted', 'married', 'ASD']\n",
    "    if encoders == None:\n",
    "        encoders = dict()\n",
    "    for col in ohe_list:\n",
    "        if col in encoders:\n",
    "            x, encoders[col] = onehot_encoder(x, col, encoder=encoders[col])\n",
    "        else:\n",
    "            x, encoders[col] = onehot_encoder(x, col)\n",
    "\n",
    "    # log (outlier)\n",
    "    log_list = [\n",
    "        'introelapse', 'testelapse', 'surveyelapse'\n",
    "    ]\n",
    "    for col in log_list:\n",
    "        x[f'log_{col}'] = np.log(x[col] + 1e-08)\n",
    "\n",
    "    # min-max cut (outlier)\n",
    "    outlier_dict = {\n",
    "        'age': {'min': 1, 'max': 100},\n",
    "        'log_introelapse': {'min': 0, 'max': 12},\n",
    "        'log_testelapse': {'min': 3, 'max': 8},\n",
    "        'log_surveyelapse': {'min': 0, 'max': 10},\n",
    "        'familysize': {'min': 1, 'max': 40}}\n",
    "    for col in outlier_dict:\n",
    "        x[col][x[col] > outlier_dict[col]['max']] = outlier_dict[col]['max']\n",
    "        x[col][x[col] < outlier_dict[col]['min']] = outlier_dict[col]['min']\n",
    "\n",
    "    # drop\n",
    "    drop_list = [\n",
    "        'index', \n",
    "        'country_AGO', 'country_ALA', 'country_ARM', 'country_AZE',\n",
    "        'country_BHS', 'country_BLR', 'country_BRB', 'country_BRN',\n",
    "        'country_BWA', 'country_DOM', 'country_ETH', 'country_FRO',\n",
    "        'country_GRL', 'country_GTM', 'country_GUF', 'country_GUY',\n",
    "        'country_IRQ', 'country_KAZ', 'country_KHM', 'country_LBY',\n",
    "        'country_LUX', 'country_MAC', 'country_MDV', 'country_MNP',\n",
    "        'country_MOZ', 'country_MUS', 'country_MWI', 'country_NAM',\n",
    "        'country_NPL', 'country_OMN', 'country_PAN', 'country_SDN',\n",
    "        'country_SSD', 'country_TUN', 'country_UGA', 'country_VGB',\n",
    "        'country_VIR', 'country_FSM', 'country_GEO', 'country_PNG',\n",
    "        'country_RWA', 'country_SYR', 'country_LAO', 'country_MNG',\n",
    "        'country_CUW', 'country_MLT', 'country_BHR', 'country_MDG'\n",
    "    ] + log_list + ohe_list\n",
    "    for col in drop_list:\n",
    "        x = x.drop(columns=col)\n",
    "\n",
    "    if scaler == None:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(x)\n",
    "    x = pd.DataFrame(scaler.transform(x), columns=x.columns, index=list(x.index.values))\n",
    "\n",
    "    return x, encoders, scaler\n",
    "\n",
    "X_train, encoders, scaler = preprocess(X_train, copy=False)\n",
    "X_target, _, _ = preprocess(target, encoders=encoders, scaler=scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8957459642672488"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators = 2048\n",
    "criterion = 'entropy'\n",
    "max_depth = 200\n",
    "min_samples_split = 3\n",
    "min_samples_leaf = 1\n",
    "min_weight_fraction_leaf = 0.0\n",
    "max_features = 'log2'\n",
    "bootstrap = False\n",
    "class_weight = None\n",
    "\n",
    "rfc = RandomForestClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    criterion=criterion,\n",
    "    max_depth=max_depth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "    max_features=max_features,\n",
    "    bootstrap=bootstrap,\n",
    "    class_weight=class_weight,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtraTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.895223545125524"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators = 2048\n",
    "criterion = 'entropy'\n",
    "max_depth = 200\n",
    "min_samples_split = 3\n",
    "min_samples_leaf = 1\n",
    "min_weight_fraction_leaf = 0.0\n",
    "max_features = 'sqrt'\n",
    "bootstrap = False\n",
    "class_weight = 'balanced_subsample'\n",
    "\n",
    "etc = ExtraTreesClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    criterion=criterion,\n",
    "    max_depth=max_depth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "    max_features=max_features,\n",
    "    bootstrap=bootstrap,\n",
    "    class_weight=class_weight,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 16 #\n",
    "booster = 'gbtree' #\n",
    "learning_rate = 0.1\n",
    "min_split_loss = 0\n",
    "max_depth = 20\n",
    "min_child_weight = 1\n",
    "max_delta_step = 0\n",
    "subsample = 0.7 #\n",
    "sampling_method = 'uniform' #\n",
    "colsample_bytree = 1\n",
    "colsample_bylevel = 1\n",
    "colsample_bynode = 1\n",
    "sketch_eps = 0.03\n",
    "scale_pos_weight = 1\n",
    "refresh_leaf = 1\n",
    "grow_policy = 'depthwise' #\n",
    "max_leaves = 0\n",
    "max_bin = 256 #\n",
    "num_parallel_tree = 16 #\n",
    "objective = 'reg:squarederror' #\n",
    "tree_method='gpu_hist' #\n",
    "gpu_id=0 #\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    booster=booster,\n",
    "    learning_rate=learning_rate,\n",
    "    min_split_loss=min_split_loss,\n",
    "    max_depth=max_depth,\n",
    "    min_child_weight=min_child_weight,\n",
    "    max_delta_step=max_delta_step,\n",
    "    subsample=subsample,\n",
    "    sampling_method=sampling_method,\n",
    "    colsample_bytree=colsample_bytree,\n",
    "    colsample_bylevel=colsample_bylevel,\n",
    "    colsample_bynode=colsample_bynode,\n",
    "    sketch_eps=sketch_eps,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    refresh_leaf=refresh_leaf,\n",
    "    grow_policy=grow_policy,\n",
    "    max_leaves=max_leaves,\n",
    "    max_bin=max_bin,\n",
    "    num_parallel_tree=num_parallel_tree,\n",
    "    objective=objective,\n",
    "    tree_method=tree_method,\n",
    "    gpu_id=gpu_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(estimators=[('rtc',\n",
       "                                RandomForestClassifier(bootstrap=False,\n",
       "                                                       criterion='entropy',\n",
       "                                                       max_depth=200,\n",
       "                                                       max_features='log2',\n",
       "                                                       min_samples_split=3,\n",
       "                                                       n_estimators=2048,\n",
       "                                                       n_jobs=-1)),\n",
       "                               ('etc',\n",
       "                                ExtraTreesClassifier(class_weight='balanced_subsample',\n",
       "                                                     criterion='entropy',\n",
       "                                                     max_depth=200,\n",
       "                                                     max_features='sqrt',\n",
       "                                                     min_samples_split=3,\n",
       "                                                     n_estimators=2048,\n",
       "                                                     n_jobs=-1))],\n",
       "                   final_estimator=LogisticRegression())"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [('rtc', rfc),('etc', etc)]\n",
    "\n",
    "clf = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict_proba(X_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97195323, 0.02804677],\n",
       "       [0.09136011, 0.90863989],\n",
       "       [0.09477749, 0.90522251],\n",
       "       ...,\n",
       "       [0.04720648, 0.95279352],\n",
       "       [0.96097567, 0.03902433],\n",
       "       [0.28483593, 0.71516407]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['nerdiness'] = predictions[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(outputs + '/ensemble_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tensorflow2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0d89f1864a9bf14cab8f26b7808b33bb77607a1d02657216be50ded1f9f2bf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
